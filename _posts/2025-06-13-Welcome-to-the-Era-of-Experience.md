---
title: "Welcome to the Era of Experience 中譯版"
date: 2025-06-13
categories:
  - ai
tags:
  - 翻譯
  - translation
---

## Welcome to the Era of Experience 歡迎來到經驗時代

**David Silver, Richard S. Sutton***

《歡迎來到經驗時代》為即將在 MIT 出版社出版的《Designing an Intelligence》一書中的一章預印本。本文被稱作《苦澀的教訓》續集 ，Open AI 工程師 Jason Wei，Chain-of-Thought 論文第一作者，在 2024 貼出的每日作息表，在 09:45 都會複習一次《苦澀的教訓》。[原文出處](https://storage.googleapis.com/deepmind-media/Era-of-Experience%20/The%20Era%20of%20Experience%20Paper.pdf)

### 摘要

我們正處於人工智慧新時代的門檻，這個時代有望達到前所未有的能力水平。新一代的智能體將主要透過**經驗學習**，獲得超越人類的能力。本篇筆記探討了定義這個即將到來的時代的關鍵特徵。

---

### 人類數據時代

近年來，人工智慧（AI）透過**在大量人類生成數據上進行訓練**，並**利用人類專家範例和偏好進行微調**，取得了顯著的進步。大型語言模型（LLMs）就是這種方法的典範，它們實現了廣泛的通用性。單個 LLM 現在可以執行從寫詩、解決物理問題到診斷醫療問題和總結法律文件等各種任務。

然而，儘管模仿人類足以將許多人類能力複製到合格的水平，但這種方法單獨來看，尚未也可能無法在許多重要主題和任務上實現**超越人類的智慧**。在數學、編碼和科學等關鍵領域，從人類數據中提取的知識正在迅速接近極限。大多數高品質的數據來源——那些能夠實際提高強大智能體性能的數據——要麼已經被消耗，要麼即將被消耗。僅靠從人類數據進行監督學習所推動的進步速度明顯放緩，這表明需要一種**新的方法**。此外，有價值的新見解，例如新的定理、技術或科學突破，超越了目前人類理解的界限，無法透過現有的人類數據捕捉。

---

### 經驗時代

為了取得顯著的進一步進展，需要**新的數據來源**。這些數據必須以一種方式生成，即隨著智能體變得更強大而持續改進；任何靜態的合成數據生成程序將很快被超越。這可以透過讓智能體從**自身經驗**中持續學習來實現，即由智能體與其環境互動所產生的數據。人工智慧正處於一個新時期的開端，在這個時期，經驗將成為改進的主導媒介，並最終使當今系統中使用的人類數據規模相形見絀。

這場轉變可能已經開始，即使是那些代表著以人類為中心的人工智慧的大型語言模型也是如此。一個例子是數學能力。AlphaProof [20] 最近成為第一個在**國際數學奧林匹克競賽**中獲得獎牌的程式，超越了以人類為中心的方法 [27, 19] 的表現。AlphaProof 的強化學習（RL）演算法¹最初接觸了約十萬個由人類數學家多年創造的形式化證明，隨後透過與形式化證明系統的持續互動，**又生成了一億個**。這種對互動經驗的專注，使得 AlphaProof 能夠探索超越現有形式化證明範疇的數學可能性，從而發現新穎且具挑戰性問題的解決方案。非形式化數學也透過用**自我生成數據**取代專家生成數據而取得了成功；例如，DeepSeek 最近的工作「強調了強化學習的力量和美妙之處：我們不需要明確教導模型如何解決問題，我們只需提供它正確的激勵，它就能自主地發展出高級的問題解決策略。」[10]

我們認為，一旦**經驗學習的全部潛力被釋放**，將會產生令人難以置信的新能力。這個經驗時代的特徵將是：智能體和環境除了從大量的經驗數據中學習之外，還將在以下幾個維度上**突破以人類為中心的人工智慧系統的限制**：

* 智能體將生活在**經驗流**中，而不是短暫的互動片段。
* 它們的行動和觀察將**豐富地根植於環境**中，而不僅僅透過人類對話進行互動。
* 它們的獎勵將**根植於它們對環境的經驗**，而不是來自人類的預先判斷。
* 它們將**規劃和/或推理經驗**，而不僅僅以人類的術語進行推理。

我們相信，當今的技術，只要選擇適當的演算法，就已足以提供強大的基礎來實現這些突破。此外，人工智慧社群對這一議程的追求將激發這些方向上的新創新，從而迅速推動人工智慧走向**真正超越人類的智能體**。

---

### 經驗流（Streams）

經驗型智能體可以在其**一生中持續學習**。在人類數據時代，基於語言的人工智慧主要關注短暫的互動回合：例如，用戶提出一個問題，智能體（或許經過幾個思考步驟或工具使用動作後）做出回應。通常，很少或沒有訊息從一個回合延續到下一個回合，這排除了隨著時間推移的任何適應。此外，智能體只專注於當前回合內的結果，例如直接回答用戶的問題。相較之下，人類（和其他動物）則存在於**持續多年的行動和觀察流**中。訊息貫穿整個經驗流，其行為從過去的經驗中進行自我修正和改進。此外，目標可以根據遠在未來才會發生的行動和觀察來指定。例如，人類可能會選擇行動來實現長期目標，例如改善健康、學習語言或實現科學突破。

強大的智能體應該擁有**自己的經驗流**，並像人類一樣在長期時間尺度上不斷進步。這將使智能體能夠採取行動以實現未來的目標，並隨著時間的推移不斷適應新的行為模式。例如，一個連接到用戶穿戴裝置的健康與保健智能體可以監測數個月的睡眠模式、活動水平和飲食習慣。然後，它可以根據長期趨勢和用戶特定的健康目標，提供個性化建議、鼓勵並調整其指導。同樣，一個個性化教育智能體可以追蹤用戶學習新語言的進度，找出知識缺口，適應他們的學習風格，並在數月甚至數年內調整其教學方法。此外，一個科學智能體可以追求雄心勃勃的目標，例如發現新材料或減少二氧化碳。這樣的智能體可以在長時間內分析現實世界的觀察結果，開發和運行模擬，並建議現實世界的實驗或干預措施。

在每個案例中，智能體都採取一系列步驟，以**最大化長期成功**，達到其指定的目標。單一步驟可能不會提供任何即時好處，甚至短期內可能有害，但仍然可能總體上有助於長期成功。這與目前的人工智慧系統形成強烈對比，這些系統對請求提供即時回應，卻無法衡量或優化其行動對環境的未來影響。

---

### 行動與觀察

經驗時代的智能體將在現實世界中**自主行動**。人類數據時代的 LLMs 主要關注人類特權的行動和觀察，即向用戶輸出文本，並從用戶那裡輸入文本回智能體。這與自然智慧截然不同，在自然智慧中，動物透過**運動控制和感測器**與環境互動。儘管動物，尤其是人類，可以與其他動物交流，但這種交流是透過與其他感覺運動控制相同的介面進行的，而不是透過特權通道。

長期以來，人們已經認識到 LLMs 也可以在數位世界中執行行動，例如透過呼叫 API（參見 [43]）。最初，這些能力主要來自人類工具使用的範例，而不是來自智能體自身的經驗。然而，編碼和工具使用能力越來越依賴於**執行反饋** [17, 7, 12]，即智能體實際運行程式碼並觀察發生的情況。最近，新一波的原型智能體已經開始以更通用的方式與電腦互動，透過使用人類操作電腦所使用的相同介面 [3, 15, 24]。這些變化預示著從**僅限人類特權的通訊**轉向**更自主的互動**，智能體能夠在世界中獨立行動。這樣的智能體將能夠主動探索世界，適應不斷變化的環境，並發現人類可能永遠不會想到的策略。這些更豐富的互動將提供一種自主理解和控制數位世界的方式。智能體可以使用「人類友善」的行動和觀察，例如使用者介面，這些介面自然地促進與使用者的溝通和協作。智能體也可以採取「機器友善」的行動，執行程式碼並呼叫 API，使智能體能夠自主地為其目標服務。在經驗時代，智能體還將透過數位介面與現實世界互動。例如，一個科學智能體可以監測環境感測器、遠端操作望遠鏡，或控制實驗室中的機械臂以自主進行實驗。

---

### 獎勵（Rewards）

如果經驗型智能體能夠從**外部事件和信號中學習**，而不僅僅是人類偏好呢？以人類為中心的 LLMs 通常根據**人類的預先判斷**來優化獎勵：專家觀察智能體的行動並判斷它是否是好的行動，或者從多個選項中選出最好的智能體行動。例如，專家可能會判斷健康智能體的建議、教育助理的教學，或科學家智能體建議的實驗。事實上，這些獎勵或偏好是由人類在沒有考慮其後果的情況下決定的，而不是衡量這些行動對環境的影響，這意味著它們**沒有直接根植於世界的現實**。以這種方式依賴人類的預先判斷通常會導致智能體性能達到**無法逾越的天花板**：智能體無法發現人類評估者所低估的更好策略。為了發現遠遠超出現有人類知識的新想法，必須使用**根植於環境本身的信號**：**具體化獎勵**。

例如，一個健康助理可以將使用者的健康目標轉換為基於其**靜止心率、睡眠時間和活動水平**等多種信號的獎勵，而一個教育助理則可以使用**考試成績**來為語言學習提供具體化獎勵。同樣地，一個旨在減少全球暖化的科學智能體可能會使用基於二氧化碳水平的**經驗觀察**來設定獎勵，而一個旨在發現更堅固材料的目標則可以基於**材料模擬器測量結果**（例如拉伸強度或楊氏模量）的組合來具體化獎勵。

**具體化獎勵**可能來自作為智能體環境一部分的人類。²例如，人類用戶可以報告他們是否覺得蛋糕美味、運動後有多疲憊，或者頭痛的程度，從而讓助理智能體提供更好的食譜、改進其健身建議，或改善其推薦的藥物。這些獎勵衡量智能體在其環境中行動的後果，並最終應該比預先判斷提議蛋糕食譜、運動計畫或治療計畫的人類專家提供更好的協助。

如果獎勵不是來自人類數據，那它們從何而來？一旦智能體透過豐富的行動和觀察空間與世界連接（見上文），就不會缺少**具體化信號**來作為獎勵的基礎。事實上，世界充滿了各種量化指標，如**成本、錯誤率、飢餓、生產力、健康指標、氣候指標、利潤、銷售、考試成績、成功、訪問量、產量、庫存、點讚數、收入、快樂/痛苦、經濟指標、準確性、能量、距離、速度、效率或能量消耗**。此外，還有無數額外的信號來自於特定事件的發生，或來自原始觀察和行動序列中衍生的特徵。

原則上，可以創建各種不同的智能體，每個智能體都將一個具體化的信號作為其獎勵進行優化。有人認為，即使是**一個簡單的獎勵信號**，只要高效地優化，可能就足以激發廣泛的能力 [34]。這是因為在複雜環境中實現一個簡單目標，通常需要掌握各種各樣的技能。

然而，追求單一獎勵信號表面上似乎不能滿足通用人工智慧的需求，即能夠可靠地引導其行為符合任意用戶所需。那麼，對**具體化、非人類獎勵信號的自主優化**是否與現代人工智慧系統的需求相矛盾呢？我們認為不一定如此，並勾勒出一種可能滿足這些期望的方法；其他方法也可能存在。

這個想法是**靈活地適應獎勵**，根據具體化信號，以用戶引導的方式。例如，獎勵函數可以由一個神經網絡定義，該網絡將智能體與用戶和環境的互動作為輸入，並輸出一個標量獎勵。這使得獎勵能夠根據用戶的目標來選擇或組合環境中的信號。例如，用戶可能指定一個廣泛的目標，例如「改善我的健康」，獎勵函數可能會返回用戶心率、睡眠時間和步數的函數。或者用戶可能指定一個「幫助我學習西班牙語」的目標，獎勵函數可以返回用戶的西班牙語考試成績。

此外，用戶可以在學習過程中提供回饋，例如他們的滿意度，這可以用來**微調獎勵函數**。獎勵函數隨後可以隨著時間的推移進行調整，以改進其選擇或組合信號的方式，並識別和糾正任何偏差。這也可以理解為一個**兩級優化過程**，將用戶回饋作為頂層目標進行優化，並在底層優化來自環境的具體化信號。⁴透過這種方式，少量的人類數據可以促進大量的自主學習。

---

### 規劃與推理

經驗時代會改變智能體**規劃和推理**的方式嗎？最近，在使用大型語言模型（LLMs）進行推理或「思考」方面取得了顯著進展 [23, 14, 10]，它們透過遵循**思維鏈**（chain of thought）來輸出回應 [16]。從概念上講，LLMs 可以充當**通用電腦** [30]：一個 LLM 可以將 token 附加到其自己的上下文，允許它在輸出最終結果之前執行任意演算法。

在人類數據時代，這些推理方法被明確設計為**模仿人類思維過程**。例如，LLMs 被提示生成類人思維鏈 [16]、模仿人類思維痕跡 [42]，或強化與人類範例相符的思維步驟 [18]。推理過程可以進一步微調，以產生與人類專家確定為正確答案的思維痕跡 [44]。

然而，人類語言不太可能提供**通用電腦的最佳實例**。更有效率的思維機制肯定存在，使用非人類語言，例如可能利用符號、分佈式、連續或可微分計算。一個自我學習系統原則上可以透過從經驗中學習如何思考來發現或改進這些方法。例如，AlphaProof 學會了以與人類數學家截然不同的方式正式證明複雜定理 [20]。

此外，通用電腦的原理只解決了智能體的內部計算；它沒有將其與外部世界的現實聯繫起來。一個被訓練模仿人類思想或甚至匹配人類專家答案的智能體可能會繼承那些**深植於數據中的錯誤思維方法**，例如有缺陷的假設或內在的偏見。例如，如果一個智能體被訓練使用五千年前人類的思想和專家答案來推理，它可能會以**泛靈論**來推理物理問題；一千年前它可能以**神學術語**來推理；三百年前它可能以**牛頓力學**來推理；而五十年前則以**量子力學**來推理。超越每種思維方法都需要與真實世界互動：提出假設、進行實驗、觀察結果，並相應地更新原則。同樣，一個智能體必須**根植於真實世界的數據**，才能推翻錯誤的思維方法。這種根植提供了**反饋迴路**，讓智能體能夠根據現實測試其繼承的假設，並發現不受當前主導人類思維模式限制的新原則。如果沒有這種根植，一個智能體，無論多麼複雜，都將成為現有人類知識的回音室。為了超越這一點，智能體必須積極參與世界，收集觀察數據，並利用這些數據迭代地完善其理解，這在許多方面反映了推動人類科學進步的過程。

將思維直接根植於外部世界的一種可能方式是建立**世界模型** [37]，該模型預測智能體行動對世界的影響，包括預測獎勵。例如，一個健康助理可能會考慮推薦一個當地健身房或健康播客。智能體的世界模型可能會預測用戶的心率或睡眠模式在採取此行動後會如何變化，以及預測與用戶的未來對話。這允許智能體直接**根據自身的行動及其對世界的因果效應進行規劃** [36, 29]。隨著智能體在其經驗流中繼續與世界互動，其動力學模型不斷更新，以糾正其預測中的任何錯誤。給定一個世界模型，智能體可以應用可擴展的規劃方法來提高智能體的預測性能。

規劃和推理方法並非相互排斥：智能體可以在規劃期間應用內部 LLM 計算來選擇每個行動，或模擬和評估這些行動的後果。

---

### 為何是現在？

從經驗中學習並非新鮮事。強化學習系統此前已掌握了大量複雜任務，這些任務在模擬器中以清晰的獎勵信號呈現（參見圖 1 中的「模擬時代」）。例如，強化學習方法透過自我對弈在棋盤遊戲中達到或超越人類表現，例如西洋雙陸棋 [39]、圍棋 [31]、西洋棋 [32]、撲克 [22, 6] 和戰略遊戲 [26]；電玩遊戲如 Atari [21]、星海爭霸 II [40]、Dota 2 [4] 和跑車浪漫旅 [41]；靈巧操控任務如魔術方塊 [1]；以及資源管理任務如資料中心散熱 [13]。此外，AlphaZero [33] 等強大的強化學習智能體展現出令人印象深刻且可能無限擴展的能力，隨著神經網路規模、互動經驗數量和思考時間的增加而擴展。然而，基於這種範式的智能體未能跨越模擬（封閉問題，具有單一、精確定義的獎勵）到現實（開放式問題，具有看似定義不清的多元獎勵）之間的鴻溝。

人類數據時代提供了一個誘人的解決方案。大量的人類數據語料庫包含了各種任務的自然語言範例。在這些數據上訓練的智能體獲得了廣泛的能力，而模擬時代的成功則更為狹隘。因此，經驗性強化學習的方法學在很大程度上被拋棄，轉而支持更通用的智能體，導致向以人類為中心的人工智慧的廣泛轉變。

然而，在這場轉變中，一些東西被遺失了：**智能體自我發現知識的能力**。例如，AlphaZero 發現了西洋棋和圍棋的全新策略，改變了人類下這些棋局的方式 [28, 45]。**經驗時代**將使這種能力與人類數據時代所達到的任務通用性水平相結合。正如上文所述，當智能體能夠在現實世界的經驗流中**自主行動和觀察** [11]，並且獎勵可以靈活地與任何豐富的**現實世界具體化信號**相連接時，這將成為可能。能夠與複雜現實世界行動空間互動的自主智能體 [3, 15, 24] 的出現，以及能夠解決豐富推理空間中開放式問題的強大強化學習方法 [20, 10]，都預示著向經驗時代的轉變迫在眉睫。

---

### 強化學習方法

強化學習（RL）擁有悠久的歷史，其根源深植於**自主學習**，即智能體透過與環境的直接互動來自學。早期的 RL 研究產生了一系列強大的概念和演算法。例如，**時序差分學習** [35] 使智能體能夠估計未來獎勵，從而帶來了諸如西洋雙陸棋中超越人類表現的突破 [39]。出於樂觀或好奇心而驅動的**探索技術**被開發出來，以幫助智能體發現創新的新行為並避免陷入次優常規 [2]。Dyna 演算法等方法使智能體能夠建立並從其世界模型中學習，從而能夠規劃和推理未來的行動 [36, 29]。**選項（options）**和**選項內/選項間學習（inter/intra-option learning）**等概念促進了**時間抽象**，使智能體能夠在更長的時間尺度上進行推理，並將複雜任務分解為可管理的分目標 [38]。

然而，以人類為中心的 LLMs 的興起，將焦點從**自主學習**轉向**利用人類知識**。RLHF（從人類回饋中進行強化學習）[9, 25] 等技術，以及使語言模型與人類推理對齊的方法 [44]，被證明極其有效，推動了人工智慧能力的快速發展。這些方法儘管強大，但通常繞過了核心的 RL 概念：RLHF 透過用人類專家取代機器估計值來規避了價值函數的需求，來自人類數據的強大先驗知識減少了對探索的依賴，而以人類為中心的推理則減少了對世界模型和時間抽象的需求。

然而，可以說，這種範式轉變是**因噎廢食**。雖然以人類為中心的強化學習實現了前所未有的行為廣度，但也對智能體的性能施加了**新的天花板**：智能體無法超越現有的人類知識。此外，人類數據時代主要關注設計用於**短暫、無根基的人機互動**的強化學習方法，不適合**長期的、根植於現實的自主互動流**。

經驗時代為重新審視和改進經典強化學習概念提供了機會。這個時代將帶來**思考獎勵函數的新方式**，這些獎勵函數將靈活地根植於觀察數據。它將重新審視**價值函數**以及從**長期、尚未完整的序列中估計它們的方法**。它將帶來**原則性且實用的現實世界探索方法**，這些方法能夠發現與人類先驗知識截然不同的新行為。將開發捕捉根植互動複雜性的**新型世界模型**。**時間抽象的新方法**將使智能體能夠在經驗方面，在越來越長的時間範圍內進行推理。透過在強化學習的基礎上發展，並使其核心原則適應這個新時代的挑戰，我們將能夠**釋放自主學習的全部潛力**，並為**真正超越人類的智慧**鋪平道路。

---

### 結果

經驗時代的到來，即人工智慧智能體從與世界互動中學習，預示著一個與我們過去所見截然不同的未來。這種新範式在提供巨大潛力的同時，也帶來了重要的風險和挑戰，需要仔細考慮，包括但不限於以下幾點。

從積極面來看，經驗學習將解鎖前所未有的能力。在日常生活中，個性化助理將利用連續的經驗流，在數月或數年內，根據個人的健康、教育或專業需求，適應其長期目標。或許最具變革性的是**科學發現的加速**。人工智慧智能體將在材料科學、醫學或硬體設計等領域自主設計並進行實驗。透過不斷從自身實驗結果中學習，這些智能體可以以前所未有的速度迅速探索新的知識前沿，從而開發出新型材料、藥物和技術。

然而，這個新時代也帶來了重大且新穎的挑戰。儘管自動化人類能力有望提高生產力，但這些改進也可能導致**工作崗位流失**。智能體甚至可能展現出以前被認為是人類專屬領域的能力，例如**長期問題解決、創新以及對現實世界後果的深刻理解**。

此外，儘管任何人工智慧都存在潛在濫用的普遍擔憂，但能夠**自主與世界長時間互動以實現長期目標的智能體**可能會帶來更高的風險。預設情況下，這減少了人類干預和調節智能體行動的機會，因此需要**高度的信任和責任**。擺脫人類數據和人類思維模式也可能使未來的人工智慧系統**更難以解釋**。

然而，儘管我們承認經驗學習會增加某些安全風險，並且無疑需要進一步研究以確保安全過渡到經驗時代，我們也應該認識到它可能也會提供一些重要的安全好處。

首先，一個經驗豐富的智能體會感知其所處的環境，其行為可以隨著時間的推移適應環境的變化。任何預先編程的系統，包括固定的AI系統，都可能不了解其環境上下文，並會變得不適應其部署的變化世界。例如，一個關鍵硬體可能會發生故障，一場流行病可能會導致社會迅速變化，或者一項新的科學發現可能會引發一連串的快速技術發展。相比之下，經驗豐富的智能體可以觀察並學會規避故障的硬體，適應快速的社會變化，或者擁抱並基於新的科學和技術進行建設。或許更重要的是，智能體可以識別其行為何時引起人類的擔憂、不滿或困擾，並**自適應地修改其行為以避免這些負面後果**。

其次，智能體的獎勵函數本身可以透過經驗進行調整，例如使用之前描述的**兩級優化**（參見獎勵）。重要的是，這意味著不一致的獎勵函數通常可以透過**試錯法隨著時間的推移逐漸糾正**。例如，獎勵函數可以根據人類擔憂的跡象進行修改，而不是盲目地優化一個信號，例如最大化迴紋針數量 [5]，這樣就可以避免迴紋針生產消耗地球所有資源的情況。這類似於人類如何為彼此設定目標，然後如果他們觀察到人們鑽系統漏洞、忽視長期福祉或造成不良的負面後果，就會調整這些目標；儘管也像人類設定目標一樣，不能保證完全對齊。

最後，依賴**物理經驗的進步**本質上受到在現實世界中執行行動和觀察其後果所需時間的限制。例如，新藥的開發，即使有 AI 輔助設計，仍然需要無法一蹴而就的真實世界試驗。這可能會對潛在的 AI 自我改進速度形成天然的制動。

---

### 結論

**經驗時代**標誌著人工智慧發展的關鍵時刻。基於當今堅實的基礎，但超越了人類衍生數據的局限性，智能體將越來越多地從與世界的互動中學習。智能體將透過豐富的觀察和行動**自主地與環境互動**。它們將在**終身經驗流**的過程中持續適應。它們的目標將能夠**指向任何具體化信號的組合**。此外，智能體將利用強大的**非人類推理**，並構建**基於智能體行動對其環境影響的規劃**。最終，經驗數據將超越人類生成數據的規模和品質。

這場**範式轉變**，伴隨著強化學習中的演算法進步，將在許多領域**解鎖超越任何人類所擁有的新能力**。

---

### 致謝

作者感謝 Thomas Degris、Rohin Shah、Tom Schaul 和 Hado van Hasselt 提供的寶貴意見和討論。

### 參考文獻

[1] I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas, J. Schneider, N. Tezak, J. Tworek, P. Welinder, L. Weng, Q. Yuan, W. Zaremba, and L. Zhang. Solving Rubik's cube with a robot hand, 2019.
[2] S. Amin, M. Gomrokchi, H. Satija, H. van Hoof, and D. Precup. A survey of exploration methods in reinforcement learning, 2021.
[3] Anthropic. Introducing computer use, a new Claude 3.5 Sonnet, and Claude 3.5 Haiku. [https://www.anthropic.com/news/3-5-models-and-computer-use](https://www.anthropic.com/news/3-5-models-and-computer-use), 2024.
[4] C. Berner, G. Brockman, B. Chan, V. Cheung, P. Debiak, C. Dennison, D. Farhi, Q. Fischer, S. Hashme, C. Hesse, R. Jozefowicz, S. Gray, C. Olsson, J. Pachocki, M. Petrov, H. P. d. O. Pinto, J. Raiman, T. Salimans, J. Schlatter, J. Schneider, S. Sidor, I. Sutskever, J. Tang, F. Wolski, and S. Zhang. Dota 2 with large scale deep reinforcement learning, 2019.
[5] N. Bostrom. Ethical issues in advanced artificial intelligence. [https://nickbostrom.com/ethics/ai](https://nickbostrom.com/ethics/ai), 2003.
[6] N. Brown and T. Sandholm. Sup
